
## 1Ô∏è‚É£ Problem Statement & Motivation
Most DevOps learning projects available publicly focus on **tool demonstrations**, not **system thinking**.  
They usually stop at:
- Running containers
- Simple CI pipelines
- One-click Kubernetes deployments

However, real DevOps roles require engineers to:
- Make **design decisions**
- Handle **constraints**
- Face **failures**
- Balance **cost, security, and scalability**
- Operate systems with **limited resources**

### Motivation Behind AFM
The **AFM (App Feature / Microservice) Project** was created to:
- Simulate **real enterprise DevOps challenges**
- Build a system **incrementally**, not perfectly
- Start small ‚Üí evolve ‚Üí migrate ‚Üí optimize
- Document **why decisions were taken**, not just _what was done_

This project intentionally embraces **constraints**:
- Limited budget
- No domain/DNS
- Single-node Kubernetes cluster
- Manual + automated workflows
- Real failures and recovery

---

## 2Ô∏è‚É£ Why Microservices Architecture?
### Why Microservices (Instead of Monolith)?
Microservices were chosen to demonstrate:
- Independent build & deploy
- Service isolation
- Kubernetes-native workflows
- Real CI/CD complexity
This mirrors how **banking and enterprise systems** are actually built.

---

## 3Ô∏è‚É£ Why Only 4 AFMs (Microservices)?
The project intentionally limits itself to **four AFMs** to maintain **clarity and depth**:
### Selected AFMs:
1. **Auth Service**
2. **Registration Service**
3. **Login Service**
4. **Frontend UI (separate microservice)**    

### Why This Split?
- **Auth, Login, Registration**  
    ‚Üí Represent typical identity workflows in banking systems
- **Frontend UI as a separate microservice**  
    ‚Üí Enables:
    - Independent UI deployment
    - Backend changes without UI rebuild
    - Real ingress & routing scenarios

This is **not over-engineering**, but **controlled realism**.

---

## 4Ô∏è‚É£ Why Spring Boot (Java) & HTML/CSS Frontend?
### Backend ‚Äì Spring Boot (Java)
Chosen because:
- Widely used in banking & enterprise systems
- Strong ecosystem
- Easy observability integration
- Realistic for interviews

### Frontend ‚Äì HTML/CSS (Simple)
Chosen intentionally:
- Focus stays on **DevOps**, not frontend frameworks
- Faster iteration
- Easy containerization
- Clear ingress routing demonstration

---

## 5Ô∏è‚É£ Technology Stack ‚Äì Why Each Tool Was Chosen
### ‚òÅÔ∏è AWS ‚Äì Why AWS?
AWS was selected because it reflects **real enterprise adoption**.
**Services used and why:**
- **EC2** ‚Äì GitLab shell runner & initial hosting
- **S3** ‚Äì Terraform remote state
- **DynamoDB** ‚Äì Terraform state locking
- **ECR** ‚Äì Secure container registry
- **IAM** ‚Äì Fine-grained access control
- **CloudWatch** ‚Äì Infrastructure-level monitoring
- **RDS** ‚Äì Persistent production-style database

---

### ‚ò∏Ô∏è Amazon EKS ‚Äì Why Single Node (t3.medium)?
- Budget-constrained, realistic learning setup
- Forces **capacity planning**
- Exposes pod scheduling issues
- Enables real troubleshooting
> Single-node EKS is **harder**, not easier ‚Äî and that was intentional.

---

## 6Ô∏è‚É£ GitLab for SCM & CI/CD ‚Äì Why?
### Why GitLab Instead of GitHub Actions / Jenkins?
- Unified SCM + CI/CD
- Enterprise-grade pipelines
- Strong support for:
    - Manual gates
    - Parameterized pipelines        
    - Multi-stage DevSecOps workflows

---

### Why EC2 t3.medium as GitLab Shell Runner?
- Needed Docker-in-Docker control
- Needed full system access
- Same host used initially for app hosting
- Reduced cost & complexity

### Challenges Faced (Will be shown later):
- Disk cleanup
- Docker daemon conflicts
- Permission issues
- Long-running pipelines

(All documented in later sections)

---

## 7Ô∏è‚É£ Monorepo Strategy ‚Äì Why One Repo?
All AFM microservices were initially kept in a **single monorepo**.
### Why Monorepo?
- Easier dependency management
- Shared pipeline logic
- Centralized control    
- Faster iteration

### Challenge:
> How to build & deploy **only one service** without rebuilding all?

### Solution:
- **Hybrid GitLab pipeline**
- Manual inputs:
    - build = true/false
    - deploy = true/false
    - service selection
- Real enterprise-style pipeline control

---

## 8Ô∏è‚É£ Terraform ‚Äì Infrastructure as Code
### Why Terraform?
- Cloud-agnostic IaC
- Declarative
- Enterprise standard

### Key Practices Used:
- Modular design
- Environment-aware structure
- Remote state:
    - **S3 backend**
    - **DynamoDB lock**
- No local `terraform apply`
- Infra changes only via pipeline

---

## 9Ô∏è‚É£ Docker ‚Äì Why Still Required:
Even with Kubernetes:
- Docker is the **packaging standard**
- Enables:
    - Local testing
    - Docker Compose workflows
    - Consistent builds across environments

---

## üîü Kubernetes ‚Äì Why Not Just Docker Compose:
### Why Kubernetes?
Docker Compose was initially sufficient, **until**:
- Scaling was needed
- Rolling updates were needed
- Health checks mattered
- Ingress routing became complex
- Future upgrade strategies were discussed

Kubernetes enabled:
- Declarative deployments
- Self-healing
- Traffic abstraction
- Production-style operations

---

## 1Ô∏è‚É£1Ô∏è‚É£ HTTPS Journey ‚Äì Docker Compose ‚Üí EKS
### Phase 1: Docker Compose
- NGINX
- Self-signed certificate
- HTTPS enabled locally
- Good for early-stage validation

### Phase 2: EKS + ALB Controller
- Switched to AWS Load Balancer Controller
- Self-signed cert **not supported**
- AWS ACM not used (no DNS)
- Operated over HTTP

This shows **real constraint-based decision making**.

---

## 1Ô∏è‚É£2Ô∏è‚É£ Database Evolution ‚Äì user.json ‚Üí RDS
### Initial State:
- Local `/data/user.json`
- Simple & fast

### Why Changed?
- Not persistent
- Not scalable
- Not production-like

### Final State:
- Amazon RDS
- Secure access via SG
- Used by all AFMs
- Real database connectivity challenges handled

---

## 1Ô∏è‚É£3Ô∏è‚É£ DevSecOps ‚Äì Shift Left Security
### Why Security Early?
Security added **inside pipelines**, not post-deployment.
### Tools Chosen:
- **SAST** ‚Äì SonarQube
- **SCA / Image Scan** ‚Äì Trivy
- **DAST** ‚Äì OWASP ZAP

### Integration:
- Multi-stage GitLab pipelines
- Fail-on-critical issues
- Manual gates where needed
---

## 1Ô∏è‚É£4Ô∏è‚É£ Observability ‚Äì Why PT, GA & CloudWatch?
### CloudWatch
- Node & infra metrics
- AWS-native visibility

### Prometheus (PT)
- Kubernetes & app metrics
- Service-level monitoring

### Grafana (GA)
- Visualization
- Debugging
- Interview-grade dashboards

### Real Challenge Faced:
- Single node capacity exceeded
- 17 pods couldn‚Äôt schedule
- Solution:
    - Reduced AFM replicas
    - Kept observability components minimal

---

## 1Ô∏è‚É£5Ô∏è‚É£ Why Separate Repos & Pipelines?
### 1. **afm-project**
- Application CI/CD
- Build, scan, deploy

### 2. **afm-infra**
- Terraform modules
- Environment-wise provisioning
- IAM, EKS, ALB

### 3. **afm-observability**
- Monitoring stack
- Metrics, dashboards
- Verification pipelines
This separation reflects **real platform teams**.

---

## 1Ô∏è‚É£6Ô∏è‚É£ Journey Summary (Very Important)

- Started with EC2 + Docker Compose
- Added NGINX + HTTPS
- Migrated to Kubernetes (EKS)
- Faced IAM & EKS issues
- Integrated DevSecOps
- Hit capacity limits
- Optimized pod strategy
- Built observability
- Documented everything

# üì∏ Architecture Diagrams & Screenshots
This section provides **visual evidence** of the AFM project‚Äôs architecture, pipelines, deployments, security scans, and observability setup.

> ‚ö†Ô∏è Screenshots are intentionally organized **by pipeline and phase**, not randomly.  
> This mirrors how DevOps platforms are reviewed in real teams.

---

## üìê Architecture Diagrams
### 1Ô∏è‚É£ High-Level System Architecture
**Description:**
- User ‚Üí ALB ‚Üí EKS ‚Üí AFM Microservices ‚Üí RDS
- Clear separation of Infra, App, Security, Observability

`![AFM High-Level Architecture](diagrams/afm-high-level-architecture.png)`

üìå _Diagram will include_:
- AWS VPC
- EKS single-node cluster
- ALB Controller
- AFM microservices
- RDS
- Monitoring stack

---

### 2Ô∏è‚É£ CI/CD Architecture (GitLab-Centric)
**Description:**
- Separate pipelines for:
    - afm-project
    - afm-infra
    - afm-observability
- Security and monitoring integrated

`![AFM CI/CD Architecture](diagrams/afm-cicd-architecture.png)`

---

## üß± Infrastructure Pipeline Screenshots (afm-infra)
### 3Ô∏è‚É£ Terraform Plan Stage
`![Terraform Plan](screenshots/afm-infra/terraform-plan.png)`
**Shows:**
- Environment-based execution
- Planned infra changes
- No direct apply without review

---

### 4Ô∏è‚É£ Terraform Apply (EKS Provisioning)
`![Terraform Apply](screenshots/afm-infra/terraform-apply.png)`
**Highlights:**
- EKS cluster creation
- IAM roles
- ALB controller readiness

---

### 5Ô∏è‚É£ EKS Cluster Validation
`![EKS Nodes](screenshots/afm-infra/eks-nodes.png)`
**Shows:**
- Single node (t3.medium)
- Node ready state
- Capacity awareness

---

## üöÄ Application Pipeline Screenshots (afm-project)
### 6Ô∏è‚É£ Hybrid GitLab Pipeline Inputs
`![Hybrid Pipeline Inputs](screenshots/afm-project/hybrid-pipeline-inputs.png)`
**Shows:**
- Build toggle
- Deploy toggle
- Single-service selection
- Monorepo challenge solution

---

### 7Ô∏è‚É£ Docker Build & Push to ECR
`![Docker Build Push](screenshots/afm-project/docker-build-push.png)`

---

### 8Ô∏è‚É£ Kubernetes Deployment Rollout
`![K8s Deployment](screenshots/afm-project/k8s-deployment.png)`

**Shows:**
- Rolling updates
- Zero downtime behavior
- Pod recreation

---

### 9Ô∏è‚É£ ALB Ingress Routing
`![ALB Ingress](screenshots/afm-project/alb-ingress.png)`
**Highlights:**
- Path-based routing
- Service-level exposure
- HTTP traffic via ALB

---

## üîê DevSecOps Screenshots
### üîü SonarQube ‚Äì SAST Scan
`![SonarQube Scan](screenshots/security/sonarqube-scan.png)`
---

### 1Ô∏è‚É£1Ô∏è‚É£ Trivy ‚Äì Image Vulnerability Scan

`![Trivy Scan](screenshots/security/trivy-scan.png)`

---

### 1Ô∏è‚É£2Ô∏è‚É£ OWASP ZAP ‚Äì DAST Scan
`![OWASP ZAP Scan](screenshots/security/zap-scan.png)`

**Shows:**

- Runtime security testing
- Pipeline-integrated DAST
- Shift-left security implementation

---

## üìä Observability Screenshots (afm-observability)
### 1Ô∏è‚É£3Ô∏è‚É£ Prometheus Targets & ServiceMonitors

`![Prometheus Targets](screenshots/observability/prometheus-targets.png)`

---

### 1Ô∏è‚É£4Ô∏è‚É£ Grafana Dashboards

`![Grafana Dashboard](screenshots/observability/grafana-dashboard.png)`

---

### 1Ô∏è‚É£5Ô∏è‚É£ CloudWatch Metrics

`![CloudWatch Metrics](screenshots/observability/cloudwatch-metrics.png)`

---

## ‚ö†Ô∏è Real Issues & Constraints (Visual Proof)

### 1Ô∏è‚É£6Ô∏è‚É£ Pod Scheduling Failure (Capacity Exceeded)

`![Pod Scheduling Issue](screenshots/issues/pod-scheduling-failure.png)`

**Explanation:**
- Single-node cluster
- 17 pods
- Insufficient CPU/memory
- Real production-style limitation

---

### 1Ô∏è‚É£7Ô∏è‚É£ Resolution ‚Äì Replica Optimization

`![Replica Fix](screenshots/issues/replica-optimization.png)`

**Shows:**
- Reduced replicas
- Successful scheduling
- Stability restored
